---
layout: default
title: "Treasure Hunt Deep Q Learning – Algorithms and Data Structures"
---

[![Generic badge](https://img.shields.io/badge/language-Python_3.x-blue.svg)](https://www.python.org) 
[![Generic badge](https://img.shields.io/badge/library-Keras_&_NumPy-cyan.svg)](https://keras.io/) 
[![Generic badge](https://img.shields.io/badge/environment-Jupyter_Notebook-orange.svg)](https://jupyter.org/) 
[![Generic badge](https://img.shields.io/badge/category-Algorithms_and_Data_Structures-lightgrey.svg)](#) 
[![Generic badge](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

# Treasure Hunt Deep Q Learning Project  
CS 370 Current and Emerging Trends in Computer Science  
**Category:** Algorithms and Data Structures  

---

## Artifact Overview

The artifact I selected for this milestone is my **Treasure Hunt Deep Q Learning** project from **CS 370**. In the original assignment, I was given partial starter code where only the structure of the Q learning training loop existed. My main task was to complete the logic inside the `qtrain()` function.

The project simulates a pirate agent trying to reach a treasure inside an eight by eight maze using reinforcement learning. The assignment included two Python classes, `TreasureMaze.py` and `GameExperience.py`, plus a Jupyter Notebook. The notebook showed how the model was built and how a full training run should work, but the most important algorithmic part of the code was left incomplete.

For the **CS 499 capstone**, I enhanced this artifact so that it now meets the **Algorithms and Data Structures** category requirements and better demonstrates my skills in this area.

<div style="text-align: center;">
    <p>
        <img src="assets/img/cs370-maze-environment.png" width="360px" />&nbsp;&nbsp;&nbsp;
        <img src="assets/img/cs370-agent-path.png" width="360px" />
    </p>
    <p><em>Figure 1. Treasure maze environment and example agent path toward the treasure.</em></p>
</div>

---

## Why I Selected This Artifact for My ePortfolio

I selected this artifact because it clearly shows my growth in **algorithm design** and my ability to use **algorithmic principles** to solve real problems. Enhancing this project allowed me to take an incomplete algorithm, finish it, optimize it, and then explain the improvements in clear language.

This artifact highlights several of my skills:

- My understanding of **reinforcement learning** and **Q learning**  
- My ability to use a **replay memory structure** that behaves like a queue for storing experience  
- My ability to evaluate algorithm performance using **time complexity**  
- My skill in improving **convergence** and stabilizing learning through better training control  
- My ability to write **clear, professionally documented code** that another person can follow  

Because it combines algorithm design, data structures, optimization, and neural network decision making, this artifact is a strong fit for the Algorithms and Data Structures category in my ePortfolio.

---

## How the Artifact Was Improved

I enhanced the artifact in four important ways during the capstone.

### 1. Epsilon Adjustment Based on Performance

In the original version, epsilon stayed at a fixed value for exploration. In the enhanced version, I adjusted the exploration rate so that once the agent maintains a high recent win rate, epsilon is reduced to a smaller value. This means:

- Early in training, the agent explores more and gathers varied experiences  
- After performance improves, the agent relies more on its learned policy and explores less  

This change improves convergence and reduces wasted random moves once the agent already understands the maze.

### 2. Optimized Experience Replay Memory Usage

The project uses a **replay memory** to store state, action, reward, next state, and game status. For the capstone, I improved how and when the model trains by using a **controlled mini batch schedule**:

- I allow the replay memory to fill to a useful size first  
- I train the model using mini batches pulled from the memory instead of training on every single step  

This controlled training schedule reduces unnecessary computation and makes the learning process more stable because the model sees more diverse batches instead of highly correlated samples.

<div style="text-align: center;">
    <p>
        <img src="assets/img/cs370-qtrain-code.png" width="360px" />&nbsp;&nbsp;&nbsp;
        <img src="assets/img/cs370-replay-buffer.png" width="360px" />
    </p>
    <p><em>Figure 2. Enhanced <code>qtrain()</code> logic and replay memory usage in the Jupyter Notebook.</em></p>
</div>

### 3. Added Algorithmic Time Complexity Documentation

To show that I can reason about performance, I added clear **time complexity notes** in the header comments and inside the code. I documented approximate complexity for:

- The main **training loop**  
- **Mini batch sampling** from replay memory  
- **Model prediction** operations  

These notes show that I can evaluate computing solutions using algorithmic principles and explain the cost of each main step in simple terms.

### 4. Improved Code Comments and Readability

I rewrote the comment section in the `qtrain()` function using **simple, human friendly language**. Each block of logic now has comments that describe:

- What the code does  
- Why it is needed for the learning process  
- How it connects to reinforcement learning ideas such as exploration, exploitation, and convergence  

This improvement supports the communication outcome for the capstone and makes the algorithm much easier to understand for another student or for a future version of myself.

---

## Training Output Demonstrating Convergence

As part of the enhancement, I ran the full training session and captured output that shows stable loss and a high win rate. The enhanced training loop reaches a sustained win rate near one hundred percent and then passes the completion check.

<div style="text-align: center;">
    <img src="assets/img/cs370-training-output.png" width="700px" />
    <p><em>Figure 3. Training output showing stable loss and achievement of one hundred percent win rate at epoch 471.</em></p>
</div>

This output acts as visual evidence that the algorithm is learning an effective policy for the maze and that the changes to epsilon control and replay scheduling are working as intended.

---

## Alignment to Course Outcomes

In Module One of CS 499, I planned to use this artifact to meet the **Algorithms and Data Structures** outcome. After completing the enhancement, I met this outcome by upgrading an existing algorithm, analyzing efficiency, and improving the overall design.

### Outcome 3  
**Design and evaluate computing solutions using algorithmic principles and data structures while managing trade offs.**

I met this outcome by:

- Completing the Q learning algorithm in `qtrain()`  
- Optimizing it with a **win rate based epsilon adjustment**  
- Improving the **replay memory workflow** and training schedule  
- Documenting **time complexity** and performance behavior inside the code  
- Evaluating **convergence** using loss values and win rate metrics  

### Outcome 4  
**Use innovative skills and tools in computing practices to implement solutions.**

I also supported this outcome because the project integrates:

- **Neural networks** for function approximation  
- **Replay memory** data structures for off policy learning  
- **Reinforcement learning** techniques applied to a maze navigation problem  

Together, these show that I can use modern AI tools and algorithms to build working intelligent agents.

---

## Reflection on the Enhancement Process

Enhancing this artifact helped me deepen my understanding of how reinforcement learning uses algorithmic logic to solve sequential decision problems. I saw more clearly how exploration, exploitation, replay memory, and convergence all connect in a single algorithm.

One challenge I faced was balancing randomness and learned behavior. If epsilon stayed high, the agent kept exploring even when it already knew good paths. If epsilon dropped too fast, the agent got stuck in local patterns. Implementing a **win rate based epsilon adjustment** solved this by tying exploration to real performance.

Another challenge was making the model more efficient. Training too often slowed down progress and wasted time, but training too rarely caused unstable behavior. Adding a **warm up period** and **scheduled mini batch updates** helped find a better middle point. This also made me think about the cost of each model update and how that cost relates to overall training time.

This enhancement improved my confidence in **algorithm design**, **data structure usage**, and **performance evaluation**. It also strengthened my ability to write clear, organized, and well documented code. Overall, this artifact now represents my skills in designing and improving algorithms that work in real artificial intelligence environments.

---

## Repository and Files

You can explore the enhanced artifact and supporting files in my GitHub repository:

- **Artifact repository:**  
  `[CS370 Treasure Hunt Deep Q Learning – Algorithms and Data Structures](https://github.com/your-username/CS370-TreasureHunt-DeepQLearning)`  

The repository includes:

- `TreasureHuntGame-enhanced.ipynb`  
- `TreasureMaze.py`  
- `GameExperience.py`  
- `screenshots/` folder with training and maze images  
- `README.md` for this artifact  

> Update the repository link and image paths to match your actual GitHub structure and screenshot locations.

---

## References

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. (2015). Human level control through deep reinforcement learning. *Nature, 518*(7540), 529–533. https://doi.org/10.1038/nature14236  

Sutton, R. S., and Barto, A. G. (2018). *Reinforcement learning: An introduction* (2nd ed.). MIT Press. https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf  

---

<div style="text-align: right;">
    <a href="#top">
        <button style="font-size: 10px; font-weight: 500; background: #ff6347; color: #ffffff; border-radius: 50px; border-style: solid; border-color: #ff6347; padding: 5px 8px;">
            Back to Top &#8593;
        </button>
    </a>
</div>

<!-- Anchor element placed at the top of the page -->
<div id="top"></div>

